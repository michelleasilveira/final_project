[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "",
    "text": "In this report we build and compare predictive models for diabetes using the Diabetes Health Indicators dataset (diabetes_binary_health_indicators_BRFSS2015). The original outcome variable, Diabetes_012, has three categories:\n\n0 = no diabetes\n\n1 = prediabetes\n\n2 = diabetes\n\nFor this project we follow the course instructions and work with a binary response. We define a new variable, Diabetes_binary, that indicates whether a respondent has diabetes (1) or not (0):\n\nDiabetes_binary = 1 if Diabetes_012 == 2 (diabetes)\n\nDiabetes_binary = 0 otherwise (no diabetes or prediabetes)\n\nOur goals in this document are to:\n\nCreate a training/test split of the data.\nSpecify a modeling recipe based on insights from the EDA.\nFit and tune a classification tree model.\nFit and tune a random forest model.\nUse log-loss with 5-fold cross-validation to select the best model in each family.\nCompare the final tuned models on the test set and select a winner.\n\nWe use the tidymodels framework to keep preprocessing, resampling, tuning, and evaluation organized and reproducible."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "",
    "text": "In this report we build and compare predictive models for diabetes using the Diabetes Health Indicators dataset (diabetes_binary_health_indicators_BRFSS2015). The original outcome variable, Diabetes_012, has three categories:\n\n0 = no diabetes\n\n1 = prediabetes\n\n2 = diabetes\n\nFor this project we follow the course instructions and work with a binary response. We define a new variable, Diabetes_binary, that indicates whether a respondent has diabetes (1) or not (0):\n\nDiabetes_binary = 1 if Diabetes_012 == 2 (diabetes)\n\nDiabetes_binary = 0 otherwise (no diabetes or prediabetes)\n\nOur goals in this document are to:\n\nCreate a training/test split of the data.\nSpecify a modeling recipe based on insights from the EDA.\nFit and tune a classification tree model.\nFit and tune a random forest model.\nUse log-loss with 5-fold cross-validation to select the best model in each family.\nCompare the final tuned models on the test set and select a winner.\n\nWe use the tidymodels framework to keep preprocessing, resampling, tuning, and evaluation organized and reproducible."
  },
  {
    "objectID": "Modeling.html#load-packages",
    "href": "Modeling.html#load-packages",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Load Packages",
    "text": "Load Packages\nWe begin by loading the packages needed for modeling."
  },
  {
    "objectID": "Modeling.html#read-and-prepare-the-data",
    "href": "Modeling.html#read-and-prepare-the-data",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Read and Prepare the Data",
    "text": "Read and Prepare the Data\nHere we read the same dataset used in the EDA and perform minimal cleaning so that the modeling file is self-contained. We:\n\nKeep a numeric copy of Diabetes_012.\nCreate a binary outcome Diabetes_binary (No_diabetes vs Diabetes).\nRecode Sex as a factor with labels.\nProvide interpretable labels for GenHlth and Age.\n\n\ndiabetes_raw &lt;- readr::read_csv(\n  \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\n)\n\nage_labels &lt;- c(\n  \"18–24\", \"25–29\", \"30–34\", \"35–39\",\n  \"40–44\", \"45–49\", \"50–54\", \"55–59\",\n  \"60–64\", \"65–69\", \"70–74\", \"75–79\", \"80+\"\n)\n\ndiabetes &lt;- diabetes_raw |&gt;\n  mutate(\n    Diabetes_num = Diabetes_012,\n    Diabetes_012 = factor(\n      Diabetes_012,\n      levels = c(0, 1, 2),\n      labels = c(\"No_diabetes\", \"Prediabetes\", \"Diabetes\")\n    ),\n    # Binary response for modeling\n    Diabetes_binary = if_else(Diabetes_012 == \"Diabetes\", 1, 0),\n    Diabetes_binary = factor(\n      Diabetes_binary,\n      levels = c(0, 1),\n      labels = c(\"No_diabetes\", \"Diabetes\")\n    ),\n    Sex = factor(\n      Sex,\n      levels = c(0, 1),\n      labels = c(\"Female\", \"Male\")\n    ),\n    GenHlth = factor(\n      GenHlth,\n      levels = 1:5,\n      labels = c(\"Excellent\", \"Very_good\", \"Good\", \"Fair\", \"Poor\"),\n      ordered = TRUE\n    ),\n    Age_num = Age,\n    Age = factor(\n      Age,\n      levels = 1:13,\n      labels = age_labels,\n      ordered = TRUE\n    )\n  )\n\ndiabetes |&gt; \n  count(Diabetes_binary) |&gt; \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Diabetes_binary      n  prop\n  &lt;fct&gt;            &lt;int&gt; &lt;dbl&gt;\n1 No_diabetes     218334 0.861\n2 Diabetes         35346 0.139\n\n\nThe binary outcome is imbalanced, with the “Diabetes” class representing a smaller proportion of observations. This will influence how we interpret log-loss and other metrics."
  },
  {
    "objectID": "Modeling.html#traintest-split",
    "href": "Modeling.html#traintest-split",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Train/Test Split",
    "text": "Train/Test Split\nWe now split the data into a training set (70%) and a test set (30%), stratifying on the binary outcome to preserve class proportions in each partition.\n\nset.seed(2025)\n\ndata_split &lt;- initial_split(\n  diabetes,\n  prop   = 0.7,\n  strata = Diabetes_binary\n)\n\ndata_train &lt;- training(data_split)\ndata_test  &lt;- testing(data_split)\n\nprint(data_split)\n\n&lt;Training/Testing/Total&gt;\n&lt;177575/76105/253680&gt;"
  },
  {
    "objectID": "Modeling.html#modeling-strategy-and-predictors",
    "href": "Modeling.html#modeling-strategy-and-predictors",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Modeling Strategy and Predictors",
    "text": "Modeling Strategy and Predictors\nGuided by the EDA, we choose a set of predictors that showed meaningful associations with diabetes status:\n\nHighBP, HighChol, BMI, Smoker\n\nPhysActivity, GenHlth, DiffWalk\n\nAge, Sex, Income\n\nWe build a recipe that:\n\nSpecifies the outcome and predictors.\nHandles missing values for numeric predictors via median imputation.\nHandles missing values for nominal predictors via mode imputation.\nCreates dummy variables for nominal predictors.\n\n\ndiabetes_rec &lt;- recipe(\n  Diabetes_binary ~ HighBP + HighChol + BMI + Smoker +\n    PhysActivity + GenHlth + DiffWalk +\n    Age + Sex + Income,\n  data = data_train\n) |&gt;\n  step_impute_median(all_numeric_predictors()) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\nprint(diabetes_rec)\n\nWe also set up 5-fold cross-validation on the training data, stratifying by the outcome, and define a metric set centered on log-loss (here named mn_log_loss in this version of yardstick), with accuracy and ROC AUC for additional context.\n\nset.seed(2025)\nfolds &lt;- vfold_cv(\n  data_train,\n  v      = 5,\n  strata = Diabetes_binary\n)\n\nlogloss_metrics &lt;- metric_set(mn_log_loss, accuracy, roc_auc)\n\nprint(folds)\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [142059/35516]&gt; Fold1\n2 &lt;split [142059/35516]&gt; Fold2\n3 &lt;split [142060/35515]&gt; Fold3\n4 &lt;split [142061/35514]&gt; Fold4\n5 &lt;split [142061/35514]&gt; Fold5"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\n\nModel Specification\nWe begin with a classification tree model. The key tuning parameters are:\n\ncost_complexity: controls pruning / tree complexity\n\ntree_depth: maximum depth of the tree\n\nmin_n: minimum number of observations in a terminal node\n\n\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),\n  tree_depth      = tune(),\n  min_n           = tune()\n) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nprint(tree_spec)\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = tune()\n\nComputational engine: rpart \n\n\n\n\nTuning Grid\nWe create a regular grid of hyperparameters to explore a range of tree sizes and complexities.\n\ntree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-4, -1)),\n  tree_depth(range      = c(3, 10)),\n  min_n(range           = c(10, 50)),\n  levels = 3\n)\n\nprint(tree_grid)\n\n# A tibble: 27 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1         0.0001           3    10\n 2         0.00316          3    10\n 3         0.1              3    10\n 4         0.0001           6    10\n 5         0.00316          6    10\n 6         0.1              6    10\n 7         0.0001          10    10\n 8         0.00316         10    10\n 9         0.1             10    10\n10         0.0001           3    30\n# ℹ 17 more rows\n\n\n\n\nWorkflow and Cross-Validation\nWe combine the recipe and model into a workflow and use 5-fold cross-validation to estimate performance across the grid, focusing on log-loss.\n\ntree_wf &lt;- workflow() |&gt;\n  add_recipe(diabetes_rec) |&gt;\n  add_model(tree_spec)\n\nset.seed(2025)\ntree_res &lt;- tune_grid(\n  tree_wf,\n  resamples = folds,\n  grid      = tree_grid,\n  metrics   = logloss_metrics\n)\n\nprint(tree_res)\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [81 × 7]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [81 × 7]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [81 × 7]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [81 × 7]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [81 × 7]&gt; &lt;tibble [0 × 4]&gt;\n\n\nWe examine the best-performing tree configurations according to mn_log_loss (lower is better).\n\nshow_best(tree_res, metric = \"mn_log_loss\", n = 10)\n\n# A tibble: 10 × 9\n   cost_complexity tree_depth min_n .metric     .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1         0.0001          10    50 mn_log_loss binary     0.343     5 0.00299\n 2         0.0001          10    30 mn_log_loss binary     0.343     5 0.00301\n 3         0.0001          10    10 mn_log_loss binary     0.346     5 0.00249\n 4         0.0001           6    10 mn_log_loss binary     0.356     5 0.00137\n 5         0.0001           6    30 mn_log_loss binary     0.356     5 0.00137\n 6         0.0001           6    50 mn_log_loss binary     0.356     5 0.00137\n 7         0.00316          6    10 mn_log_loss binary     0.356     5 0.00137\n 8         0.00316          6    30 mn_log_loss binary     0.356     5 0.00137\n 9         0.00316          6    50 mn_log_loss binary     0.356     5 0.00137\n10         0.00316         10    10 mn_log_loss binary     0.356     5 0.00137\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nWe now select the single best set of hyperparameters and finalize the tree workflow.\n\nbest_tree &lt;- select_best(tree_res, metric = \"mn_log_loss\")\n\nprint(best_tree)\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1          0.0001         10    50 pre0_mod09_post0\n\nfinal_tree_wf &lt;- finalize_workflow(tree_wf, best_tree)\n\nprint(final_tree_wf)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_median()\n• step_impute_mode()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = 1e-04\n  tree_depth = 10\n  min_n = 50\n\nComputational engine: rpart \n\n\n\n\nFit Final Tree and Evaluate on the Test Set\nWe fit the finalized tree model on the full training data and evaluate its performance on the held-out test set.\n\nfinal_tree_fit &lt;- final_tree_wf |&gt;\n  fit(data = data_train)\n\ntree_probs &lt;- predict(final_tree_fit, data_test, type = \"prob\") |&gt;\n  bind_cols(data_test |&gt; select(Diabetes_binary))\n\ntree_classes &lt;- predict(final_tree_fit, data_test, type = \"class\") |&gt;\n  bind_cols(data_test |&gt; select(Diabetes_binary))\n\n# --- metrics computed with correct prediction types ---\n\n# log-loss uses probabilities\ntree_logloss &lt;- mn_log_loss(\n  tree_probs,\n  truth   = Diabetes_binary,\n  .pred_Diabetes\n)\n\n# accuracy uses class predictions\ntree_accuracy &lt;- accuracy(\n  tree_classes,\n  truth   = Diabetes_binary,\n  .pred_class\n)\n\n# ROC AUC uses probabilities\ntree_roc &lt;- roc_auc(\n  tree_probs,\n  truth   = Diabetes_binary,\n  .pred_Diabetes\n)\n\ntree_metrics &lt;- bind_rows(tree_logloss, tree_accuracy, tree_roc)\n\ntree_cm &lt;- conf_mat(\n  tree_classes,\n  truth   = Diabetes_binary,\n  estimate = .pred_class\n)\n\nprint(tree_metrics)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary         2.10 \n2 accuracy    binary         0.866\n3 roc_auc     binary         0.250\n\n\n\nprint(tree_cm)\n\n             Truth\nPrediction    No_diabetes Diabetes\n  No_diabetes       64446     9165\n  Diabetes           1055     1439"
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Random Forest",
    "text": "Random Forest\n\nModel Specification\nWe now consider a random forest model, which fits an ensemble of decision trees and averages their predictions. The main tuning parameters are:\n\nmtry: number of predictors sampled at each split\n\nmin_n: minimum number of observations in a terminal node\n\ntrees: total number of trees in the forest\n\nwe were using 1000 trees but reduced to 10 because of the computational time\n\nrf_spec &lt;- rand_forest(\n  mtry  = tune(),\n  min_n = tune(),\n  trees = 10\n) |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"classification\")\n\nprint(rf_spec)\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 10\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\n\n\nTuning Grid\nWe set up a grid for mtry and min_n. Because the number of predictors after dummy encoding is not known ahead of time, we estimate a reasonable range for mtry based on the training data.\n\n# approximate number of predictors after dummying\nprep_rec &lt;- diabetes_rec |&gt;\n  prep(training = data_train)\n\ntrain_processed &lt;- bake(prep_rec, new_data = data_train)\n\nn_pred &lt;- ncol(train_processed) - 1  # subtract outcome\n\nprint(n_pred)\n\n[1] 24\n\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, min(10, n_pred))),\n  min_n(range = c(5, 25)),\n  levels = 4\n)\n\nprint(rf_grid)\n\n# A tibble: 16 × 2\n    mtry min_n\n   &lt;int&gt; &lt;int&gt;\n 1     2     5\n 2     4     5\n 3     7     5\n 4    10     5\n 5     2    11\n 6     4    11\n 7     7    11\n 8    10    11\n 9     2    18\n10     4    18\n11     7    18\n12    10    18\n13     2    25\n14     4    25\n15     7    25\n16    10    25\n\n\n\n\nWorkflow and Cross-Validation\nWe create a workflow for the random forest model and tune it using the same 5-fold cross-validation folds and log-loss metric.\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(diabetes_rec) |&gt;\n  add_model(rf_spec)\n\nset.seed(2025)\nrf_res &lt;- tune_grid(\n  rf_wf,\n  resamples = folds,\n  grid      = rf_grid,\n  metrics   = logloss_metrics\n)\n\nprint(rf_res)\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 4\n  splits                 id    .metrics          .notes          \n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [48 × 6]&gt; &lt;tibble [0 × 4]&gt;\n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [48 × 6]&gt; &lt;tibble [0 × 4]&gt;\n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [48 × 6]&gt; &lt;tibble [0 × 4]&gt;\n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [48 × 6]&gt; &lt;tibble [0 × 4]&gt;\n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [48 × 6]&gt; &lt;tibble [0 × 4]&gt;\n\n\nWe examine the best-performing random forest configurations.\n\nshow_best(rf_res, metric = \"mn_log_loss\", n = 10)\n\n# A tibble: 10 × 8\n    mtry min_n .metric     .estimator  mean     n  std_err .config         \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;           \n 1     4    25 mn_log_loss binary     0.323     5 0.000760 pre0_mod08_post0\n 2     4    18 mn_log_loss binary     0.324     5 0.000546 pre0_mod07_post0\n 3     4    11 mn_log_loss binary     0.324     5 0.000631 pre0_mod06_post0\n 4     2    25 mn_log_loss binary     0.324     5 0.000613 pre0_mod04_post0\n 5     2     5 mn_log_loss binary     0.324     5 0.000579 pre0_mod01_post0\n 6     2    18 mn_log_loss binary     0.324     5 0.000766 pre0_mod03_post0\n 7     2    11 mn_log_loss binary     0.325     5 0.000688 pre0_mod02_post0\n 8     4     5 mn_log_loss binary     0.327     5 0.000876 pre0_mod05_post0\n 9     7    25 mn_log_loss binary     0.336     5 0.00183  pre0_mod12_post0\n10     7    18 mn_log_loss binary     0.341     5 0.00185  pre0_mod11_post0\n\n\nWe then select the best tuning parameters and finalize the workflow.\n\nbest_rf &lt;- select_best(rf_res, metric = \"mn_log_loss\")\n\nprint(best_rf)\n\n# A tibble: 1 × 3\n   mtry min_n .config         \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;           \n1     4    25 pre0_mod08_post0\n\nfinal_rf_wf &lt;- finalize_workflow(rf_wf, best_rf)\n\nprint(final_rf_wf)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_median()\n• step_impute_mode()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 4\n  trees = 10\n  min_n = 25\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\n\n\nFit Final Random Forest and Evaluate on the Test Set\nWe fit the finalized random forest on the full training data and evaluate it on the test set.\n\nfinal_rf_fit &lt;- final_rf_wf |&gt;\n  fit(data = data_train)\n\nrf_probs &lt;- predict(final_rf_fit, data_test, type = \"prob\") |&gt;\n  bind_cols(data_test |&gt; select(Diabetes_binary))\n\nrf_classes &lt;- predict(final_rf_fit, data_test, type = \"class\") |&gt;\n  bind_cols(data_test |&gt; select(Diabetes_binary))\n\n# --- metrics with correct prediction types ---\n\n# log-loss uses probabilities\nrf_logloss &lt;- mn_log_loss(\n  rf_probs,\n  truth   = Diabetes_binary,\n  .pred_Diabetes\n)\n\n# accuracy uses class predictions\nrf_accuracy &lt;- accuracy(\n  rf_classes,\n  truth   = Diabetes_binary,\n  .pred_class\n)\n\n# ROC AUC uses probabilities\nrf_roc &lt;- roc_auc(\n  rf_probs,\n  truth   = Diabetes_binary,\n  .pred_Diabetes\n)\n\nrf_metrics &lt;- bind_rows(rf_logloss, rf_accuracy, rf_roc)\n\nrf_cm &lt;- conf_mat(\n  rf_classes,\n  truth    = Diabetes_binary,\n  estimate = .pred_class\n)\n\nprint(rf_metrics)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 mn_log_loss binary         2.75 \n2 accuracy    binary         0.865\n3 roc_auc     binary         0.181\n\n\n\nprint(rf_cm)\n\n             Truth\nPrediction    No_diabetes Diabetes\n  No_diabetes       64627     9398\n  Diabetes            874     1206"
  },
  {
    "objectID": "Modeling.html#final-model-comparison",
    "href": "Modeling.html#final-model-comparison",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Final Model Comparison",
    "text": "Final Model Comparison\nWe now compare the test set performance of the tuned classification tree and random forest models. Our primary metric is log-loss (reported here as mn_log_loss, lower is better), but we also consider accuracy and ROC AUC.\n\ntree_summary &lt;- tree_metrics |&gt;\n  mutate(model = \"Classification tree\")\n\nrf_summary &lt;- rf_metrics |&gt;\n  mutate(model = \"Random forest\")\n\ncomparison &lt;- bind_rows(tree_summary, rf_summary) |&gt;\n  select(model, .metric, .estimate) |&gt;\n  arrange(.metric, desc(.estimate))\n\nprint(comparison)\n\n# A tibble: 6 × 3\n  model               .metric     .estimate\n  &lt;chr&gt;               &lt;chr&gt;           &lt;dbl&gt;\n1 Classification tree accuracy        0.866\n2 Random forest       accuracy        0.865\n3 Random forest       mn_log_loss     2.75 \n4 Classification tree mn_log_loss     2.10 \n5 Classification tree roc_auc         0.250\n6 Random forest       roc_auc         0.181\n\n\nTypically we expect the random forest to outperform a single tree on log-loss and ROC AUC due to its ensemble nature and better ability to capture complex interactions. However reducing the tree parameters in the random forest model = 10 caused the random forest to be slightly worse than the classification tree, so likely this will be our champion model."
  },
  {
    "objectID": "Modeling.html#conclusion",
    "href": "Modeling.html#conclusion",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Conclusion",
    "text": "Conclusion\nIn this modeling analysis we:\n\nCreated a binary diabetes outcome from the original three-level Diabetes_012 variable.\nSplit the data into stratified training and test sets.\nBuilt a preprocessing recipe that handles missing values and encodes categorical predictors.\nTuned a classification tree model using 5-fold cross-validation and log-loss.\nTuned a random forest model using the same resampling scheme and metrics.\nCompared the tuned tree and random forest on the test set.\n\nBased on the test set results, we select the model with the lowest log-loss (and typically higher ROC AUC) as our final model. This chosen/champion model will be refit on the full dataset and exposed via a plumber API in the next stage of the project."
  },
  {
    "objectID": "Modeling.html#saving-champion-model",
    "href": "Modeling.html#saving-champion-model",
    "title": "Diabetes Health Indicators – Modeling",
    "section": "Saving champion model",
    "text": "Saving champion model\n\n# Make sure the 'model' directory exists\nif (!dir.exists(\"model\")) dir.create(\"model\")\n\n# Print champion model (optional)\nprint(final_tree_fit)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_impute_median()\n• step_impute_mode()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 177575 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 177575 24742 No_diabetes (0.86066732 0.13933268)  \n     2) HighBP&lt; 0.5 101438  6160 No_diabetes (0.93927325 0.06072675) *\n     3) HighBP&gt;=0.5 76137 18582 No_diabetes (0.75593995 0.24406005)  \n       6) GenHlth_1&lt; 0.1581139 55922 10469 No_diabetes (0.81279282 0.18720718)  \n        12) GenHlth_1&lt; -0.1581139 28633  3525 No_diabetes (0.87689030 0.12310970)  \n          24) BMI&lt; 32.5 23381  2415 No_diabetes (0.89671100 0.10328900) *\n          25) BMI&gt;=32.5 5252  1110 No_diabetes (0.78865194 0.21134806)  \n            50) HighChol&lt; 0.5 2644   389 No_diabetes (0.85287443 0.14712557) *\n            51) HighChol&gt;=0.5 2608   721 No_diabetes (0.72354294 0.27645706)  \n             102) DiffWalk&lt; 0.5 2180   550 No_diabetes (0.74770642 0.25229358) *\n             103) DiffWalk&gt;=0.5 428   171 No_diabetes (0.60046729 0.39953271)  \n               206) Sex_Male&lt; 0.5 275    96 No_diabetes (0.65090909 0.34909091)  \n                 412) Income&gt;=3.5 212    65 No_diabetes (0.69339623 0.30660377) *\n                 413) Income&lt; 3.5 63    31 No_diabetes (0.50793651 0.49206349)  \n                   826) PhysActivity&gt;=0.5 33    12 No_diabetes (0.63636364 0.36363636) *\n                   827) PhysActivity&lt; 0.5 30    11 Diabetes (0.36666667 0.63333333) *\n               207) Sex_Male&gt;=0.5 153    75 No_diabetes (0.50980392 0.49019608)  \n                 414) Age_10&gt;=0.06757444 70    25 No_diabetes (0.64285714 0.35714286) *\n                 415) Age_10&lt; 0.06757444 83    33 Diabetes (0.39759036 0.60240964) *\n        13) GenHlth_1&gt;=-0.1581139 27289  6944 No_diabetes (0.74553850 0.25446150)  \n          26) BMI&lt; 31.5 17264  3577 No_diabetes (0.79280584 0.20719416) *\n          27) BMI&gt;=31.5 10025  3367 No_diabetes (0.66413965 0.33586035)  \n            54) Age_01&lt; 0.1111874 4482  1109 No_diabetes (0.75256582 0.24743418)  \n             108) HighChol&lt; 0.5 2125   345 No_diabetes (0.83764706 0.16235294) *\n             109) HighChol&gt;=0.5 2357   764 No_diabetes (0.67585914 0.32414086)  \n               218) DiffWalk&lt; 0.5 1886   562 No_diabetes (0.70201485 0.29798515) *\n               219) DiffWalk&gt;=0.5 471   202 No_diabetes (0.57112527 0.42887473)  \n                 438) BMI&lt; 33.5 107    34 No_diabetes (0.68224299 0.31775701) *\n                 439) BMI&gt;=33.5 364   168 No_diabetes (0.53846154 0.46153846)  \n                   878) Age_12&gt;=-0.3077051 141    56 No_diabetes (0.60283688 0.39716312)  \n                    1756) BMI&lt; 42.5 87    27 No_diabetes (0.68965517 0.31034483) *\n                    1757) BMI&gt;=42.5 54    25 Diabetes (0.46296296 0.53703704) *\n                   879) Age_12&lt; -0.3077051 223   111 Diabetes (0.49775785 0.50224215)  \n                    1758) Sex_Male&lt; 0.5 150    69 No_diabetes (0.54000000 0.46000000) *\n                    1759) Sex_Male&gt;=0.5 73    30 Diabetes (0.41095890 0.58904110) *\n            55) Age_01&gt;=0.1111874 5543  2258 No_diabetes (0.59263936 0.40736064)  \n             110) HighChol&lt; 0.5 1928   647 No_diabetes (0.66441909 0.33558091)  \n               220) Sex_Male&lt; 0.5 1091   336 No_diabetes (0.69202566 0.30797434) *\n               221) Sex_Male&gt;=0.5 837   311 No_diabetes (0.62843489 0.37156511)  \n                 442) BMI&lt; 39.5 690   239 No_diabetes (0.65362319 0.34637681) *\n                 443) BMI&gt;=39.5 147    72 No_diabetes (0.51020408 0.48979592)  \n                   886) Income&gt;=3.5 123    56 No_diabetes (0.54471545 0.45528455)  \n                    1772) Age_06&lt; 0.1845419 79    31 No_diabetes (0.60759494 0.39240506) *\n                    1773) Age_06&gt;=0.1845419 44    19 Diabetes (0.43181818 0.56818182) *\n                   887) Income&lt; 3.5 24     8 Diabetes (0.33333333 0.66666667) *\n\n...\nand 96 more lines.\n\n# Save final fitted tree workflow and the recipe\nsaveRDS(final_tree_fit, \"model/final_tree_fit.rds\")\nsaveRDS(diabetes_rec, \"model/recipe.rds\")\n\nmessage(\"Champion model saved successfully!\")"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "",
    "text": "The data analyzed in this report come from the Diabetes Health Indicators dataset\n(diabetes_binary_health_indicators_BRFSS2015). Each row corresponds to a survey respondent and includes a diabetes status indicator (Diabetes_012), along with a variety of health and lifestyle indicators such as blood pressure, BMI, physical activity, smoking status, general health, and socio-demographic variables.\nIn this version of the dataset, the variable:\n\nDiabetes_012 is coded as:\n\n0 = no diabetes\n1 = prediabetes\n2 = diabetes\n\n\nOur goals in this EDA are to:\n\nDescribe the variables that will be used in the subsequent modeling.\nUnderstand the distribution of the response variable (Diabetes_012).\nExplore how key predictors relate to diabetes status.\nIdentify potential data quality issues (missing values, unusual distributions).\nBuild intuition that will guide model selection in the Modeling document.\n\nThe ultimate goal is to build a predictive model related to diabetes status, which will be developed in the modeling page/report."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "",
    "text": "The data analyzed in this report come from the Diabetes Health Indicators dataset\n(diabetes_binary_health_indicators_BRFSS2015). Each row corresponds to a survey respondent and includes a diabetes status indicator (Diabetes_012), along with a variety of health and lifestyle indicators such as blood pressure, BMI, physical activity, smoking status, general health, and socio-demographic variables.\nIn this version of the dataset, the variable:\n\nDiabetes_012 is coded as:\n\n0 = no diabetes\n1 = prediabetes\n2 = diabetes\n\n\nOur goals in this EDA are to:\n\nDescribe the variables that will be used in the subsequent modeling.\nUnderstand the distribution of the response variable (Diabetes_012).\nExplore how key predictors relate to diabetes status.\nIdentify potential data quality issues (missing values, unusual distributions).\nBuild intuition that will guide model selection in the Modeling document.\n\nThe ultimate goal is to build a predictive model related to diabetes status, which will be developed in the modeling page/report."
  },
  {
    "objectID": "EDA.html#load-packages",
    "href": "EDA.html#load-packages",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Load Packages",
    "text": "Load Packages\nWe begin by loading the R packages that will be used throughout the analysis.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(GGally)\n\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "EDA.html#read-and-prepare-the-data",
    "href": "EDA.html#read-and-prepare-the-data",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Read and Prepare the Data",
    "text": "Read and Prepare the Data\nNext, we read the dataset from the data/ folder and perform some initial cleaning steps (I know from Kaggle this is the cleaned sample). We keep a numeric version of the diabetes indicator for correlations, create a factor version for plots/modeling, recode Sex as a labeled factor, and provide more interpretable labels for GenHlth and Age.\n\ndiabetes_raw &lt;- readr::read_csv(\n  \"data/diabetes_binary_health_indicators_BRFSS2015.csv\"\n)\n\nage_labels &lt;- c(\n  \"18–24\", \"25–29\", \"30–34\", \"35–39\",\n  \"40–44\", \"45–49\", \"50–54\", \"55–59\",\n  \"60–64\", \"65–69\", \"70–74\", \"75–79\", \"80+\"\n)\n\ndiabetes &lt;- diabetes_raw |&gt;\n  mutate(\n    # numeric copy of the diabetes indicator\n    Diabetes_num = Diabetes_012,\n    # factor version for classification / plots\n    Diabetes_012 = factor(\n      Diabetes_012,\n      levels = c(0, 1, 2),\n      labels = c(\"No_diabetes\", \"Prediabetes\", \"Diabetes\")\n    ),\n    Sex = factor(\n      Sex,\n      levels = c(0, 1),\n      labels = c(\"Female\", \"Male\")\n    ),\n    GenHlth = factor(\n      GenHlth,\n      levels = 1:5,\n      labels = c(\"Excellent\", \"Very_good\", \"Good\", \"Fair\", \"Poor\"),\n      ordered = TRUE\n    ),\n    Age_num = Age,\n    Age = factor(\n      Age,\n      levels = 1:13,\n      labels = age_labels,\n      ordered = TRUE\n    )\n  )\n\ndiabetes\n\n# A tibble: 253,680 × 24\n   Diabetes_012 HighBP HighChol CholCheck   BMI Smoker Stroke\n   &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 No_diabetes       1        1         1    40      1      0\n 2 No_diabetes       0        0         0    25      1      0\n 3 No_diabetes       1        1         1    28      0      0\n 4 No_diabetes       1        0         1    27      0      0\n 5 No_diabetes       1        1         1    24      0      0\n 6 No_diabetes       1        1         1    25      1      0\n 7 No_diabetes       1        0         1    30      1      0\n 8 No_diabetes       1        1         1    25      1      0\n 9 Diabetes          1        1         1    30      1      0\n10 No_diabetes       0        0         1    24      0      0\n# ℹ 253,670 more rows\n# ℹ 17 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;ord&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;fct&gt;, Age &lt;ord&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;,\n#   Diabetes_num &lt;dbl&gt;, Age_num &lt;dbl&gt;"
  },
  {
    "objectID": "EDA.html#basic-data-overview",
    "href": "EDA.html#basic-data-overview",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Basic Data Overview",
    "text": "Basic Data Overview\nWe first report the number of observations and variables in the cleaned dataset.\n\nn_obs  &lt;- nrow(diabetes)\nn_vars &lt;- ncol(diabetes)\n\ntibble(\n  n_observations = n_obs,\n  n_variables    = n_vars\n)\n\n# A tibble: 1 × 2\n  n_observations n_variables\n           &lt;int&gt;       &lt;int&gt;\n1         253680          24\n\n\nWe then use skimr::skim() to obtain a high-level summary of the distributions and types of all variables.\n\nskimr::skim(diabetes)\n\n\nData summary\n\n\nName\ndiabetes\n\n\nNumber of rows\n253680\n\n\nNumber of columns\n24\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n20\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nDiabetes_012\n0\n1\nFALSE\n3\nNo_: 213703, Dia: 35346, Pre: 4631\n\n\nGenHlth\n0\n1\nTRUE\n5\nVer: 89084, Goo: 75646, Exc: 45299, Fai: 31570\n\n\nSex\n0\n1\nFALSE\n2\nFem: 141974, Mal: 111706\n\n\nAge\n0\n1\nTRUE\n13\n60–: 33244, 65–: 32194, 55–: 30832, 50–: 26314\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHighBP\n0\n1\n0.43\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▆\n\n\nHighChol\n0\n1\n0.42\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▆\n\n\nCholCheck\n0\n1\n0.96\n0.19\n0\n1\n1\n1\n1\n▁▁▁▁▇\n\n\nBMI\n0\n1\n28.38\n6.61\n12\n24\n27\n31\n98\n▇▅▁▁▁\n\n\nSmoker\n0\n1\n0.44\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▆\n\n\nStroke\n0\n1\n0.04\n0.20\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nHeartDiseaseorAttack\n0\n1\n0.09\n0.29\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nPhysActivity\n0\n1\n0.76\n0.43\n0\n1\n1\n1\n1\n▂▁▁▁▇\n\n\nFruits\n0\n1\n0.63\n0.48\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nVeggies\n0\n1\n0.81\n0.39\n0\n1\n1\n1\n1\n▂▁▁▁▇\n\n\nHvyAlcoholConsump\n0\n1\n0.06\n0.23\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nAnyHealthcare\n0\n1\n0.95\n0.22\n0\n1\n1\n1\n1\n▁▁▁▁▇\n\n\nNoDocbcCost\n0\n1\n0.08\n0.28\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nMentHlth\n0\n1\n3.18\n7.41\n0\n0\n0\n2\n30\n▇▁▁▁▁\n\n\nPhysHlth\n0\n1\n4.24\n8.72\n0\n0\n0\n3\n30\n▇▁▁▁▁\n\n\nDiffWalk\n0\n1\n0.17\n0.37\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nEducation\n0\n1\n5.05\n0.99\n1\n4\n5\n6\n6\n▁▁▅▅▇\n\n\nIncome\n0\n1\n6.05\n2.07\n1\n5\n7\n8\n8\n▁▁▃▂▇\n\n\nDiabetes_num\n0\n1\n0.30\n0.70\n0\n0\n0\n0\n2\n▇▁▁▁▁\n\n\nAge_num\n0\n1\n8.03\n3.05\n1\n6\n8\n10\n13\n▂▃▇▇▆"
  },
  {
    "objectID": "EDA.html#variables-of-interest",
    "href": "EDA.html#variables-of-interest",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Variables of Interest",
    "text": "Variables of Interest\nFor this project, we focus on a subset of predictors that are both interpretable and likely to be informative for predicting diabetes status. Here we select these variables and inspect their structure.\n\nvars_of_interest &lt;- diabetes |&gt;\n  select(\n    Diabetes_012,\n    HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,\n    HeartDiseaseorAttack, PhysActivity, Fruits, Veggies,\n    HvyAlcoholConsump, AnyHealthcare, NoDocbcCost,\n    GenHlth, MentHlth, PhysHlth, DiffWalk,\n    Sex, Age, Age_num, Education, Income,\n    Diabetes_num\n  )\n\nglimpse(vars_of_interest)\n\nRows: 253,680\nColumns: 24\n$ Diabetes_012         &lt;fct&gt; No_diabetes, No_diabetes, No_diabetes, No_diabete…\n$ HighBP               &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n$ HighChol             &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ CholCheck            &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0…\n$ Stroke               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ HeartDiseaseorAttack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PhysActivity         &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ Fruits               &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1…\n$ Veggies              &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ HvyAlcoholConsump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AnyHealthcare        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ NoDocbcCost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ GenHlth              &lt;ord&gt; Poor, Good, Poor, Very_good, Very_good, Very_good…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n$ Sex                  &lt;fct&gt; Female, Female, Female, Female, Female, Male, Fem…\n$ Age                  &lt;ord&gt; 60–64, 50–54, 60–64, 70–74, 70–74, 65–69, 60–64, …\n$ Age_num              &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…\n$ Diabetes_num         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0…"
  },
  {
    "objectID": "EDA.html#missing-data",
    "href": "EDA.html#missing-data",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Missing Data",
    "text": "Missing Data\nBefore proceeding with the analysis, we investigate the extent of missing data for each selected variable. First, we summarize the number and proportion of missing values by variable.\n\nmissing_summary &lt;- vars_of_interest |&gt;\n  select(-Diabetes_num, -Age_num) |&gt;\n  summarise(across(everything(), ~ sum(is.na(.)))) |&gt;\n  pivot_longer(everything(),\n               names_to = \"variable\",\n               values_to = \"n_missing\") |&gt;\n  mutate(prop_missing = n_missing / n_obs)\n\nmissing_summary |&gt;\n  arrange(desc(n_missing)) |&gt; \n  head(10)\n\n# A tibble: 10 × 3\n   variable             n_missing prop_missing\n   &lt;chr&gt;                    &lt;int&gt;        &lt;dbl&gt;\n 1 Diabetes_012                 0            0\n 2 HighBP                       0            0\n 3 HighChol                     0            0\n 4 CholCheck                    0            0\n 5 BMI                          0            0\n 6 Smoker                       0            0\n 7 Stroke                       0            0\n 8 HeartDiseaseorAttack         0            0\n 9 PhysActivity                 0            0\n10 Fruits                       0            0\n\n\nWe then visualize the proportion of missing values among variables that have at least one missing observation. If there are no missing values, we simply print a message indicating that the data are complete.\n\nif (any(missing_summary$n_missing &gt; 0)) {\n  missing_summary |&gt;\n    filter(n_missing &gt; 0) |&gt;\n    ggplot(aes(x = reorder(variable, prop_missing),\n               y = prop_missing)) +\n    geom_col() +\n    coord_flip() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n      x = NULL,\n      y = \"Proportion Missing\",\n      title = \"Proportion of Missing Values by Variable\"\n    )\n} else {\n  tibble(msg = \"No missing values were found in the selected variables.\") |&gt;\n    knitr::kable()\n}\n\n\n\n\nmsg\n\n\n\n\nNo missing values were found in the selected variables.\n\n\n\n\n\nThis is the cleaned dataset explained in the Kaggle, so I was not expecting any missing value."
  },
  {
    "objectID": "EDA.html#distribution-of-the-response-diabetes_012",
    "href": "EDA.html#distribution-of-the-response-diabetes_012",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Distribution of the Response: Diabetes_012",
    "text": "Distribution of the Response: Diabetes_012\nWe next examine the distribution of the diabetes status indicator to understand class balance across the three categories.\n\nresp_counts &lt;- vars_of_interest |&gt;\n  count(Diabetes_012) |&gt;\n  mutate(prop = n / sum(n))\n\nresp_counts\n\n# A tibble: 3 × 3\n  Diabetes_012      n   prop\n  &lt;fct&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 No_diabetes  213703 0.842 \n2 Prediabetes    4631 0.0183\n3 Diabetes      35346 0.139 \n\n\nWe visualize these proportions with a bar plot.\n\nresp_counts |&gt;\n  ggplot(aes(x = Diabetes_012, y = prop, fill = Diabetes_012)) +\n  geom_col(show.legend = FALSE) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Diabetes Status\",\n    y = \"Proportion of Respondents\",\n    title = \"Prevalence of Diabetes Status in the Sample\"\n  )\n\n\n\n\n\n\n\n\nThe dataset is imbalanced, with a smaller proportion of respondents labeled as diabetes compared to those without diabetes. Prediabetes appears as an intermediate category. This imbalance will be important to considerwhen evaluating models. This was mentioned in the Kaggle website"
  },
  {
    "objectID": "EDA.html#univariate-distributions-of-key-numeric-predictors",
    "href": "EDA.html#univariate-distributions-of-key-numeric-predictors",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Univariate Distributions of Key Numeric Predictors",
    "text": "Univariate Distributions of Key Numeric Predictors\nWe begin by studying the distributions of some important numeric variables: BMI, days of poor mental health, and days of poor physical health.\n\nvars_of_interest |&gt;\n  select(BMI, MentHlth, PhysHlth) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") |&gt;\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 30) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  labs(\n    x = NULL,\n    y = \"Count\",\n    title = \"Distributions of Selected Numeric Predictors\"\n  )\n\n\n\n\n\n\n\n\nBMI usually right-skewed, with most values in the 20–40 range. Mental and physical health days are often concentrated at 0, with a long right tail for respondents reporting many days of poor health."
  },
  {
    "objectID": "EDA.html#univariate-distributions-of-key-binarycategorical-predictors",
    "href": "EDA.html#univariate-distributions-of-key-binarycategorical-predictors",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Univariate Distributions of Key Binary/Categorical Predictors",
    "text": "Univariate Distributions of Key Binary/Categorical Predictors\nMany predictors in this dataset are Dummy or 0/1 indicators. Here I examine the marginal distributions for a few key binary variables, followed by a separate summary of the sex variable.\n\ncat_vars &lt;- vars_of_interest |&gt;\n  select(HighBP, HighChol, Smoker, PhysActivity,\n         Fruits, Veggies, DiffWalk)\n\ncat_vars |&gt;\n  pivot_longer(everything(),\n               names_to = \"variable\",\n               values_to = \"value\") |&gt;\n  mutate(value = factor(value)) |&gt;\n  group_by(variable, value) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  group_by(variable) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = value, y = prop, fill = value)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    x = NULL,\n    y = \"Proportion\",\n    title = \"Distributions of Selected Binary Predictors\"\n  )\n\n\n\n\n\n\n\n\n\nvars_of_interest |&gt;\n  count(Sex) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = Sex, y = prop, fill = Sex)) +\n  geom_col(show.legend = FALSE) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Sex\",\n    y = \"Proportion\",\n    title = \"Distribution of Sex\"\n  )\n\n\n\n\n\n\n\n\nThese plots show the prevalence of high blood pressure, high cholesterol, smoking, physical activity, difficulty walking, and sex in the sample."
  },
  {
    "objectID": "EDA.html#numeric-predictors-vs.-diabetes-status",
    "href": "EDA.html#numeric-predictors-vs.-diabetes-status",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Numeric Predictors vs. Diabetes Status",
    "text": "Numeric Predictors vs. Diabetes Status\nWe now turn to bivariate relationships. First, we compare BMI, mental health days, and physical health days across diabetes status groups using boxplots.\n\nvars_of_interest |&gt;\n  select(Diabetes_012, BMI, MentHlth, PhysHlth) |&gt;\n  pivot_longer(cols = c(BMI, MentHlth, PhysHlth),\n               names_to = \"variable\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(x = Diabetes_012, y = value, fill = Diabetes_012)) +\n  geom_boxplot(outlier.alpha = 0.2, show.legend = FALSE) +\n  facet_wrap(~ variable, scales = \"free_y\") +\n  labs(\n    x = \"Diabetes Status\",\n    y = NULL,\n    title = \"Numeric Predictors by Diabetes Status\"\n  )\n\n\n\n\n\n\n\n\nRespondents with diabetes tend to have higher BMI and more days of poor physical health. Differences in mental health days may also be present but could be less pronounced."
  },
  {
    "objectID": "EDA.html#binary-predictors-vs.-diabetes-status",
    "href": "EDA.html#binary-predictors-vs.-diabetes-status",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Binary Predictors vs. Diabetes Status",
    "text": "Binary Predictors vs. Diabetes Status\nFor binary predictors, we compare the proportion of “1” (Yes) within each diabetes group.\n\nbinary_vars &lt;- vars_of_interest |&gt;\n  select(Diabetes_012,\n         HighBP, HighChol, Smoker, PhysActivity,\n         Fruits, Veggies, DiffWalk)\n\nbinary_long &lt;- binary_vars |&gt;\n  pivot_longer(\n    cols = -Diabetes_012,\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) |&gt;\n  mutate(value = factor(value))\n\nbinary_long |&gt;\n  group_by(Diabetes_012, variable) |&gt;\n  summarise(prop_yes = mean(value == \"1\"), .groups = \"drop\") |&gt;\n  ggplot(aes(x = variable, y = prop_yes,\n             fill = Diabetes_012)) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    x = NULL,\n    y = \"Proportion with value = 1\",\n    fill = \"Diabetes status\",\n    title = \"Proportion of 'Yes' for Binary Predictors by Diabetes Status\"\n  )\n\n\n\n\n\n\n\n\nIndividuals with diabetes are more likely to report high blood pressure, high cholesterol, difficulty walking, and lower levels of physical activity. These patterns support the inclusion of these predictors in our models."
  },
  {
    "objectID": "EDA.html#ordinal-predictors-vs.-diabetes-status",
    "href": "EDA.html#ordinal-predictors-vs.-diabetes-status",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Ordinal Predictors vs. Diabetes Status",
    "text": "Ordinal Predictors vs. Diabetes Status\nWe now investigate how self-reported general health (GenHlth), age category, education, and income differ by diabetes status.\n\nordinal_vars &lt;- vars_of_interest |&gt;\n  select(Diabetes_012, GenHlth, Age, Education, Income) |&gt;\n  # make all ordinal predictors character so pivot_longer is happy\n  mutate(across(c(GenHlth, Age, Education, Income), as.character))\n\nordinal_long &lt;- ordinal_vars |&gt;\n  pivot_longer(\n    cols = -Diabetes_012,\n    names_to = \"variable\",\n    values_to = \"value\"\n  )\n\nordinal_long |&gt;\n  group_by(Diabetes_012, variable, value) |&gt;\n  summarise(n = n(), .groups = \"drop\") |&gt;\n  group_by(Diabetes_012, variable) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ggplot(aes(x = value, y = prop,\n             fill = Diabetes_012)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~ variable, scales = \"free_x\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Category\",\n    y = \"Proportion\",\n    fill = \"Diabetes status\",\n    title = \"Ordinal Predictors by Diabetes Status\"\n  )\n\n\n\n\n\n\n\n\nDiabetes tends to be more prevalent in older age categories, among respondents with worse self-reported general health, and potentially among those with lower income and education levels. These patterns suggest that demographic and socio-economic variables may add predictive power."
  },
  {
    "objectID": "EDA.html#correlation-among-numeric-predictors",
    "href": "EDA.html#correlation-among-numeric-predictors",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Correlation Among Numeric Predictors",
    "text": "Correlation Among Numeric Predictors\nFinally, we examine the correlation structure among numeric predictors and the diabetes indicator (using the numeric version Diabetes_num and Age_num).\n\nnumeric_for_cor &lt;- vars_of_interest |&gt;\n  select(Diabetes_num, BMI, MentHlth, PhysHlth, Age_num, Education, Income)\n\ncor_mat &lt;- cor(numeric_for_cor, use = \"pairwise.complete.obs\")\n\nround(cor_mat, 2)\n\n             Diabetes_num   BMI MentHlth PhysHlth Age_num Education Income\nDiabetes_num         1.00  0.22     0.07     0.18    0.19     -0.13  -0.17\nBMI                  0.22  1.00     0.09     0.12   -0.04     -0.10  -0.10\nMentHlth             0.07  0.09     1.00     0.35   -0.09     -0.10  -0.21\nPhysHlth             0.18  0.12     0.35     1.00    0.10     -0.16  -0.27\nAge_num              0.19 -0.04    -0.09     0.10    1.00     -0.10  -0.13\nEducation           -0.13 -0.10    -0.10    -0.16   -0.10      1.00   0.45\nIncome              -0.17 -0.10    -0.21    -0.27   -0.13      0.45   1.00\n\n\nWe also visualize this correlation matrix using GGally::ggcorr().\n\nggcorr(\n  numeric_for_cor,\n  label = TRUE,\n  label_round = 2,\n  label_size = 3\n) +\n  labs(\n    title = \"Correlation Matrix for Selected Numeric Predictors\"\n  )\n\n\n\n\n\n\n\n\nThe correlation matrix highlights the relationships among BMI, age, health days, and diabetes status. While no single predictor is perfectly correlated with diabetes, several variables show moderate associations that justify their inclusion in the modeling step."
  },
  {
    "objectID": "EDA.html#summary-and-next-steps",
    "href": "EDA.html#summary-and-next-steps",
    "title": "Diabetes Health Indicators – Exploratory Data Analysis",
    "section": "Summary and Next Steps",
    "text": "Summary and Next Steps\nIn this EDA, we:\n\nDescribed the structure of the Diabetes Health Indicators dataset.\nExplored the marginal distribution of the multi-level diabetes indicator (Diabetes_012).\nExamined key numeric, binary, and ordinal predictors and their relationships with diabetes status.\nIdentified variables that appear strongly associated with diabetes, including high blood pressure, high cholesterol, BMI, physical activity, difficulty walking, and general health, as well as age and income.\n\nThese insights will guide the construction of predictive models in the next stage, where we will fit and compare a classification tree and a random forest using the tidymodels framework, with model performance evaluated via log-loss on a held-out test set.\nClick here for the Modeling Page"
  }
]